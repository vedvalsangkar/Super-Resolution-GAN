{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"enhanced_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3700jwa-oczU","colab_type":"code","colab":{}},"source":["# https://github.com/jbhuang0604/SelfExSR\n","# https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCByjcfro98D","colab_type":"code","colab":{}},"source":["# import torch\n","# import pandas as pd\n","from torch import nn  # , optim\n","\n","\n","# from torch.nn import functional as F\n","# from torch.utils.data.dataset import Dataset\n","# from torchvision import transforms\n","# from PIL import Image\n","#\n","# from sklearn.metrics import roc_curve, auc\n","# from matplotlib import pyplot as plt\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"znRZovtVpBhR","colab_type":"code","colab":{}},"source":["#------------------------------- unit block of generator ENHANCED : squeeze and excitation network ----------------------------------------\n","\n","class BasicGenBlock(nn.Module):\n","\n","    def __init__(self, kernel_size=3, stride=1, channels=64, squeeze_factor=2, bias=True):\n","        super(BasicGenBlock, self).__init__()\n","\n","        self.k_size = kernel_size\n","        self.padding = self.k_size // 2\n","        self.stride = stride\n","        self.bias = bias\n","\n","        self.squeeze_factor = squeeze_factor\n","\n","        self.channels = channels\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        self.layers = nn.Sequential(nn.Conv2d(in_channels=self.channels,\n","                                              out_channels=self.channels,\n","                                              kernel_size=self.k_size,\n","                                              stride=self.stride,\n","                                              padding=self.padding,\n","                                              bias=self.bias),\n","                                    nn.BatchNorm2d(self.channels),\n","                                    nn.PReLU(),\n","                                    nn.Conv2d(in_channels=self.channels,\n","                                              out_channels=self.channels,\n","                                              kernel_size=self.k_size,\n","                                              stride=self.stride,\n","                                              padding=self.padding,\n","                                              bias=self.bias),\n","                                    nn.BatchNorm2d(self.channels)\n","                                    )\n","\n","        self.squeeze_net = nn.Sequential(nn.Linear(in_features=self.channels,\n","                                                   out_features=self.channels // self.squeeze_factor,\n","                                                   bias=self.bias),\n","                                         nn.Linear(in_features=self.channels // self.squeeze_factor,\n","                                                   out_features=self.channels,\n","                                                   bias=self.bias)\n","                                         )\n","\n","    def forward(self, x):\n","\n","        out = self.layers(x)\n","\n","        batch_size, channels, _, _ = x.size()\n","        sq = self.avg_pool(x).view(batch_size, channels)\n","        sq = self.squeeze_net(sq)\n","\n","        # sq = self.squeeze_net()\n","\n","        return out * sq.view(batch_size, channels, 1, 1).expand_as(out)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3rSQ5FzpHN6","colab_type":"code","colab":{}},"source":["#------------------------------------ generator for enhanced block -----------------------------------\n","\n","class Generator2(nn.Module):\n","\n","    def __init__(self, init_kernel_size=9, kernel_size=3, stride=1, channels=64, upscale_factor=2, bias=True):\n","        \"\"\"\n","        Model initializer method.\n","\n","        :param bias: Bias in system (default False).\n","        :param kernel_size: Convolution kernel size.\n","        \"\"\"\n","\n","        super(Generator2, self).__init__()\n","\n","        self.init_k_size = init_kernel_size\n","        self.init_padding = self.init_k_size // 2\n","\n","        self.k_size = kernel_size\n","        self.padding = self.k_size // 2\n","\n","        self.st = stride\n","        self.bias = bias\n","\n","        self.intrim_channels = channels\n","        self.final_channels = channels * (upscale_factor ** 2)\n","\n","        self.upscale_factor = upscale_factor\n","\n","        # self.upsample_mode = 'nearest'\n","        # r\"\"\"\n","        # Upsampling algorithm: one of ``'nearest'``, ``'linear'``, ``'bilinear'``, ``'bicubic'``x and ``'trilinear'``.\n","        # \"\"\"\n","\n","        self.init_layer = nn.Sequential(nn.Conv2d(in_channels=3,\n","                                                  out_channels=self.intrim_channels,\n","                                                  kernel_size=self.init_k_size,\n","                                                  stride=self.st,\n","                                                  padding=self.init_padding,\n","                                                  bias=self.bias),\n","                                        nn.PReLU()\n","                                        )\n","\n","        self.blocks_layer = nn.Sequential(BasicGenBlock(kernel_size=self.k_size,\n","                                                        stride=self.st,\n","                                                        channels=self.intrim_channels,\n","                                                        bias=self.bias),\n","                                          BasicGenBlock(kernel_size=self.k_size,\n","                                                        stride=self.st,\n","                                                        channels=self.intrim_channels,\n","                                                        bias=self.bias),\n","                                          BasicGenBlock(kernel_size=self.k_size,\n","                                                        stride=self.st,\n","                                                        channels=self.intrim_channels,\n","                                                        bias=self.bias),\n","                                          BasicGenBlock(kernel_size=self.k_size,\n","                                                        stride=self.st,\n","                                                        channels=self.intrim_channels,\n","                                                        bias=self.bias),\n","                                          BasicGenBlock(kernel_size=self.k_size,\n","                                                        stride=self.st,\n","                                                        channels=self.intrim_channels,\n","                                                        bias=self.bias)\n","                                          )\n","\n","        self.intrim_layer = nn.Sequential(nn.Conv2d(in_channels=self.intrim_channels,\n","                                                    out_channels=self.intrim_channels,\n","                                                    kernel_size=self.k_size,\n","                                                    stride=self.st,\n","                                                    padding=self.padding,\n","                                                    bias=self.bias),\n","                                          nn.BatchNorm2d(self.intrim_channels)\n","                                          )\n","\n","        self.pixel_layer = nn.Sequential(nn.Conv2d(in_channels=self.intrim_channels,\n","                                                   out_channels=self.final_channels,\n","                                                   kernel_size=self.k_size,\n","                                                   stride=self.st,\n","                                                   padding=self.padding,\n","                                                   bias=self.bias),\n","                                         nn.PixelShuffle(upscale_factor=self.upscale_factor),\n","                                         nn.PReLU(),\n","                                         nn.Conv2d(in_channels=self.intrim_channels,\n","                                                   out_channels=self.final_channels,\n","                                                   kernel_size=self.k_size,\n","                                                   stride=self.st,\n","                                                   padding=self.padding,\n","                                                   bias=self.bias),\n","                                         nn.PixelShuffle(upscale_factor=self.upscale_factor),\n","                                         nn.PReLU()\n","                                         )\n","        \"\"\"\n","        The input channels for both convolutions is 64 and output is 64 * (Scale_Factor ^ 2).\n","        \"\"\"\n","\n","        self.final_conv = nn.Conv2d(in_channels=self.intrim_channels,\n","                                    out_channels=3,\n","                                    kernel_size=self.init_k_size,\n","                                    stride=self.st,\n","                                    padding=self.init_padding,\n","                                    bias=self.bias)\n","\n","    def forward(self, x):\n","        skip_var = self.init_layer(x)\n","        out = self.blocks_layer(skip_var)\n","        out = self.intrim_layer(out)\n","        out = self.pixel_layer(out + skip_var)\n","        out = self.final_conv(out)\n","\n","        return out\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFd9dyEYpL6b","colab_type":"code","colab":{}},"source":["# ------------------------------------- unit block for discriminator ------------------------\n","\n","class BasicDisBlock(nn.Module):\n","\n","    def __init__(self, kernel_size=3, stride=1, in_channels=64, out_channels=64, bias=True):\n","        super(BasicDisBlock, self).__init__()\n","\n","        self.k_size = kernel_size\n","        self.padding = self.k_size // 2\n","        self.stride = stride\n","        self.bias = bias\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","\n","        self.layer = nn.Sequential(nn.Conv2d(in_channels=self.in_channels,\n","                                             out_channels=self.out_channels,\n","                                             kernel_size=self.k_size,\n","                                             stride=self.stride,\n","                                             padding=self.padding,\n","                                             bias=self.bias),\n","                                   nn.BatchNorm2d(self.in_channels),\n","                                   nn.LeakyReLU(),\n","                                   )\n","\n","    def forward(self, x):\n","        return self.layer(x)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OvEY1Z6pTtC","colab_type":"code","colab":{}},"source":["# ----------------------------------- discriminator for enhanced block -----------------------------\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, image_size=(224, 224), bias=False):\n","        super(Discriminator, self).__init__()\n","\n","        self.k_size = 3\n","        self.padding = self.k_size // 2\n","        self.bias = bias\n","\n","        # else:\n","        self.input_image_size = image_size\n","        if len(image_size) != 2:\n","            raise ValueError(\"Input Image size must be a tuple (Width x Height)\")\n","\n","        self.flattened_feat = (self.input_image_size[0] // 16) * (self.input_image_size[1] // 16)\n","\n","        self.init_layer = nn.Sequential(nn.Conv2d(in_channels=3,\n","                                                  out_channels=64,\n","                                                  kernel_size=self.k_size,\n","                                                  stride=1,\n","                                                  padding=self.padding,\n","                                                  bias=self.bias),\n","                                        nn.LeakyReLU()\n","                                        )\n","\n","        self.blocks_layer = nn.Sequential(BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=2,\n","                                                        in_channels=64,\n","                                                        out_channels=64,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=1,\n","                                                        in_channels=64,\n","                                                        out_channels=128,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=2,\n","                                                        in_channels=128,\n","                                                        out_channels=128,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=1,\n","                                                        in_channels=128,\n","                                                        out_channels=256,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=2,\n","                                                        in_channels=256,\n","                                                        out_channels=256,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=1,\n","                                                        in_channels=256,\n","                                                        out_channels=512,\n","                                                        bias=self.bias),\n","                                          BasicDisBlock(kernel_size=self.k_size,\n","                                                        stride=2,\n","                                                        in_channels=512,\n","                                                        out_channels=512,\n","                                                        bias=self.bias),\n","                                          )\n","\n","        self.linear_layer = nn.Sequential(nn.Linear(in_features=self.flattened_feat,\n","                                                    out_features=1024,\n","                                                    bias=self.bias),\n","                                          nn.LeakyReLU()\n","                                          )\n","\n","        self.classifier = nn.Sequential(nn.Linear(in_features=1024,\n","                                                  out_features=1,\n","                                                  bias=self.bias),\n","                                        nn.Sigmoid()\n","                                        )\n","\n","    def forward(self, x):\n","\n","        out = self.init_layer(x)\n","        out = self.blocks_layer(out)\n","        out = self.linear_layer(out)\n","        out = self.classifier(out)\n","\n","        return out"],"execution_count":0,"outputs":[]}]}